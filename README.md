---
title: Projeto Rag IA Professor
emoji: ğŸš€
colorFrom: red
colorTo: red
sdk: docker
app_port: 8501
tags:
- streamlit
pinned: false
short_description: Pipeline RAG para aprender na prÃ¡tica
---

# Welcome to Streamlit!

Edit `/src/streamlit_app.py` to customize this app to your heart's desire. :heart:

If you have any questions, checkout our [documentation](https://docs.streamlit.io) and [community
forums](https://discuss.streamlit.io).

# Jarbas RAG (Projeto de Estudo Pessoal)

> **Aviso**  
> Este repositÃ³rio Ã© um **projeto de estudo** feito para praticar RAG (Retrieval-Augmented Generation), Python e Streamlit.  
> **NÃ£o** Ã© um produto, nÃ£o tem garantias de estabilidade e **pode falhar** (por exemplo, com mensagens como `Killed` em mÃ¡quinas com pouca memÃ³ria).  
> O objetivo Ã© **aprender**: desmontar, testar, quebrar, consertar e entender cada peÃ§a do pipeline.
> VocÃª pode gerar respostas **com OpenAI** (janela grande e mais qualidade) **ou 100% local** (modelo leve Qwen 1.5B), sempre **nos seus prÃ³prios dados** indexados com FAISS.

---

## SumÃ¡rio
- [VisÃ£o Geral](#visÃ£o-geral)
- [Como o Jarbas funciona (para leigos)](#como-o-jarbas-funciona-para-leigos)
- [Estrutura do projeto](#-estrutura-do-projeto)
- [Modelos de GeraÃ§Ã£o: OpenAI vs Local](#modelos-de-geraÃ§Ã£o-openai-vs-local)
- [ğŸš€ Guia rÃ¡pido](#-guia-rÃ¡pido)
  - [0) Preparar ambiente](#0-preparar-ambiente)
  - [1) Colocar seus dados](#1-coloque-seus-dados)
  - [2) IngestÃ£o (normalizar e quebrar em chunks)](#2-ingestÃ£o-normalizar-e-quebrar-em-chunks)
  - [3) IndexaÃ§Ã£o (embeddings + FAISS)](#3-indexaÃ§Ã£o-embeddings--faiss)
  - [4) Subir a interface (Streamlit)](#4-subir-a-interface)
- [ğŸ§  O que acontece por baixo do capÃ´](#-o-que-acontece-por-baixo-do-capÃ´)
  - [RecuperaÃ§Ã£o](#recuperaÃ§Ã£o-valendo-para-ambos-backends)
  - [GeraÃ§Ã£o (Local vs OpenAI)](#geraÃ§Ã£o-duas-formas)
  - [Por que essas bibliotecas?](#por-que-escolhemos-essas-bibliotecas)
- [âš™ï¸ ParÃ¢metros e decisÃµes de seguranÃ§a](#ï¸-parÃ¢metros-e-decisÃµes-de-seguranÃ§a)
- [ğŸ’¡ Dicas de uso](#-dicas-de-uso)
- [Uso no Streamlit](#uso-no-streamlit)
- [ğŸ§¯ Troubleshooting](#-troubleshooting)
  - [â€œKilledâ€ / OOM](#killed-servidor-Ã©-encerrado)
  - [CUDA OOM](#runtimeerror-cuda-out-of-memory)
  - [Prompt muito longo](#token-indices-sequence-length--prompt-muito-longo)
  - [Sem OpenAI Key](#faltou-a-openai-key-do-usuÃ¡rio)
  - [Ãndice ausente](#indexerrorfaiss-no-such-file-or-directory)
- [ğŸ§ª Como reproduzir rÃ¡pido (comandos)](#-como-reproduzir-rÃ¡pido-comandos)
- [Privacidade & Custos](#privacidade--custos)
- [FAQ](#perguntas-frequentes-faq)
- [ğŸ“„ LicenÃ§a & crÃ©ditos](#-licenÃ§a--crÃ©ditos)

---

## VisÃ£o Geral

O Jarbas implementa um fluxo **RAG** clÃ¡ssico:

1. **RecuperaÃ§Ã£o** â€” Localizamos trechos relevantes dos seus documentos com **embeddings** (SentenceTransformers) e **FAISS**.
2. **Montagem de contexto** â€” Montamos um **prompt** contendo sua **pergunta** e um bloco **Contexto** com os trechos mais similares.
3. **GeraÃ§Ã£o** â€” Um **modelo de linguagem** (OpenAI *ou* um modelo **local** leve) usa esse contexto para produzir a resposta.
4. **ReferÃªncias** â€” Ao final, mostramos **quais trechos** do seu Ã­ndice foram usados.

O objetivo Ã© ser **didÃ¡tico** (explicaÃ§Ãµes passo a passo, exemplos mÃ­nimos) e **prÃ¡tico** (sem depender sempre de APIs pagas).

> âš ï¸ **EducaÃ§Ã£o/Estudo:** Este projeto nÃ£o Ã© um produto profissional.  
> Pode apresentar limitaÃ§Ãµes, especialmente no modo **Local** (modelo pequeno).

---

## Como o Jarbas funciona (para leigos)

Imagine que vocÃª tem uma â€œbibliotecaâ€ com os seus PDFs, anotaÃ§Ãµes, arquivos tÃ©cnicos.  
O Jarbas transforma tudo isso em **nÃºmeros** (chamados *embeddings*) e guarda em um â€œcatÃ¡logoâ€ rÃ¡pido (o **FAISS**).

Quando vocÃª faz uma **pergunta**:
- O Jarbas procura no catÃ¡logo os **trechos mais parecidos** com a sua pergunta.
- Junta esses trechos e cria um **Contexto**.
- Passa **Pergunta + Contexto** para um **modelo de IA** que escreve uma resposta.
- No final, mostra **de onde** (quais trechos) aquela resposta veio.

Se vocÃª escolher **OpenAI**, a IA Ã© mais esperta e tem memÃ³ria maior.  
Se vocÃª escolher **Local**, tudo roda no seu computador, sem internet, mas a IA Ã© **mais simples e limitada**.

---

## ğŸ—‚ï¸ Estrutura do projeto

```
.
â”œâ”€ .streamlit/                 # Config do Streamlit (tema, etc.)
â”œâ”€ .venv/                      # (opcional) ambiente virtual local
â”œâ”€ data/
â”‚  â”œâ”€ sources/                 # âœ coloque seus arquivos-fonte aqui (texto)
â”‚  â”œâ”€ processed/               # saÃ­das da ingestÃ£o (normalizados, chunkados)
â”‚  â””â”€ index/                   # Ã­ndice FAISS + textos/metadata usados na busca
â”œâ”€ notebooks/                  # (opcional) experimentos
â”œâ”€ src/
â”‚  â””â”€ jarbas/
â”‚     â”œâ”€ ingest/
â”‚     â”‚  â”œâ”€ ingest.py          # 1) normaliza e fatia corpus (â†’ processed/)
â”‚     â”‚  â”œâ”€ embed_index.py     # 2) cria embeddings e Ã­ndice FAISS (â†’ index/)
â”‚     â”‚  â””â”€ audit_corpus.py    # (opcional) inspeciona corpus/chunks
â”‚     â”œâ”€ rag/
â”‚     â”‚  â”œâ”€ local_backend.py   # backend RAG LOCAL (Qwen 2.5 1.5B)
â”‚     â”‚  â””â”€ openai_backend.py  # backend RAG com OpenAI (gpt-4o-mini)
â”‚     â””â”€ utils/
â”‚        â””â”€ text.py            # utilitÃ¡rios p/ limpeza/particionamento de texto
â”œâ”€ streamlit_app.py            # UI em Streamlit
â”œâ”€ requirements.txt
â”œâ”€ README.md
â””â”€ .env                        # (opcional) OpenAI API Key
```

> **Nota sobre os formatos**: o pipeline foi pensado para **texto**. Se vocÃª tiver PDFs/HTML/etc., converta para `.txt`/`.md` ou adapte `ingest.py` para o seu caso.

---

## Modelos de GeraÃ§Ã£o: OpenAI vs Local

| Aspecto             | OpenAI (gpt-4o-mini)                      | Local (Qwen 1.5B)                              |
|---------------------|-------------------------------------------|-----------------------------------------------|
| Qualidade           | Alta/estÃ¡vel                               | BÃ¡sica (modelo pequeno)                        |
| Janela de contexto  | Grande                                     | Menor (â‰ˆ 2k tokens)                            |
| Custo               | $$ (precisa de API key)                    | GrÃ¡tis (roda no seu hardware)                   |
| Privacidade         | Envia prompt Ã  OpenAI                      | 100% local                                      |
| Velocidade          | Varia (rede/latÃªncia)                      | Varia (sua CPU/GPU)                             |
| ConfiguraÃ§Ãµes UI    | AjustÃ¡veis                                 | Travadas para evitar â€œKilledâ€/OOM               |

> **RecomendaÃ§Ã£o:** Para respostas longas e robustas, use **OpenAI**.  
> Para testar sem custos e offline, use **Local**.

---

## ğŸš€ Guia rÃ¡pido

### 0) Preparar ambiente

```bash
# Python 3.10+ recomendado
python -m venv .venv
source .venv/bin/activate          # Windows: .venv\Scripts\activate

pip install -r requirements.txt
```

Se for usar a OpenAI, crie um `.env` com sua chave:
```
OPENAI_API_KEY=sk-...
```

### 1) Coloque seus dados
Adicione arquivos de **texto** em `data/sources/`. Exemplos: `.txt`, `.md`.  
(Para outros formatos, converta antes ou ajuste `ingest.py`.)

### 2) IngestÃ£o (normalizar e quebrar em chunks)
```bash
python -m src.jarbas.ingest.ingest
```
**O que acontece:** o script lÃª `data/sources/`, limpa/normaliza, fatia em **chunks** e grava em `data/processed/` (mais fÃ¡cil de embutir).

### 3) IndexaÃ§Ã£o (embeddings + FAISS)
```bash
python -m src.jarbas.ingest.embed_index
```
**O que acontece:** calcula **embeddings** (SentenceTransformers) para os chunks de `processed/` e cria o Ã­ndice **FAISS** em `data/index/` juntamente com `texts.json` e `metas.json`.

> (Opcional) Explore o corpus/chunks com:
> ```bash
> python -m src.jarbas.ingest.audit_corpus
> ```

### 4) Subir a interface
```bash
streamlit run streamlit_app.py
```
Na UI: escolha **Local** (sem chave, mais limitado) ou **OpenAI** (requer API key).  
Escreva sua pergunta e envie.

---

## ğŸ§  O que acontece por baixo do capÃ´

### RecuperaÃ§Ã£o (valendo para ambos backends)
- **Embeddings**: `sentence-transformers/all-MiniLM-L6-v2` (leve e rÃ¡pido) â†’ vetoriza a pergunta e os chunks.
- **Busca**: **FAISS** (index em `data/index/faiss.index`) retorna os `top_k` mais similares.
- **MMR (OpenAI)**: o backend OpenAI ainda aplica **MMR** para trocar parte dos top-k por trechos **diversos**, reduzindo redundÃ¢ncia.
- **Contexto**: os trechos selecionados viram um bloco **CONTEXTO** anexado ao *prompt* (com tags `[source :: chunk X]`).

### GeraÃ§Ã£o (duas formas)
- **Local (`src/jarbas/rag/local_backend.py`)**  
  Usa `Qwen/Qwen2.5-1.5B-Instruct` (via `transformers`/`pipeline` com `text-generation`).  
  Como Ã© um **modelo pequeno**, hÃ¡ **limites rÃ­gidos** de tamanho do prompt/saÃ­da. O cÃ³digo **trunca** pergunta/contexto quando necessÃ¡rio e cita as **ReferÃªncias** ao final.

- **OpenAI (`src/jarbas/rag/openai_backend.py`)**  
  Envia o *prompt* para `gpt-4o-mini` (ou outro configurado). Aqui a janela Ã© bem maior, mas ainda limitamos o **top_k** e o **tamanho do contexto** (por seguranÃ§a e custo). TambÃ©m retorna as **ReferÃªncias** ao final.

### Por que escolhemos essas bibliotecas?
- **SentenceTransformers**: embeddings de qualidade com custo baixo â†’ perfeito para protÃ³tipos.
- **FAISS**: busca vetorial extremamente rÃ¡pida e madura.
- **Transformers (Hugging Face)**: roda modelos *open* localmente.
- **OpenAI**: alternativa de alta qualidade/estabilidade quando se tem chave.
- **Streamlit**: cria **UI rÃ¡pida** para testar o RAG sem construir frontend.
- **NumPy / Torch**: base numÃ©rica e execuÃ§Ã£o acelerada (CPU/GPU).

---

## âš™ï¸ ParÃ¢metros e decisÃµes de seguranÃ§a

- **Local (Qwen 1.5B)**: parÃ¢metros **travados** na UI para evitar `Killed`/OOM.
  - `top_k = 4`, `temperature = 0.2`, `max_output_tokens ~320` (e o backend ainda ajusta dinamicamente).
- **OpenAI**: `top_k` configurÃ¡vel com **limite superior** (clamp) para evitar prompts gigantes.  
  Mesmo com janelas grandes, **muito contexto** pode degradar a qualidade e aumentar custo/latÃªncia.

> A UI libera **recursos** ao alternar de Local â†” OpenAI: fecha pipelines, limpa cache do Streamlit e, se houver GPU, chama `torch.cuda.empty_cache()`.

---

## ğŸ’¡ Dicas de uso

- **FaÃ§a perguntas objetivas** (1â€“3 frases) e focadas em um tÃ³pico.
- Se a resposta local vier fraca, **use o motor OpenAI** (quando possÃ­vel).
- **Curadoria do corpus** importa: remova lixo, duplicatas e textos nÃ£o informativos.
- **Chunks menores** (com sobreposiÃ§Ã£o) tendem a recuperar passagens mais precisas.
- Ajuste `ingest.py`/`utils/text.py` para o **seu domÃ­nio** (regras de limpeza, splits, metadados).

## Uso no Streamlit

1. **Escolha do motor**
   - **OpenAI (gpt-4o-mini)** â€” exige **API key** (`sk-...`), pode ajustar `top_k`, `temperatura`, `tokens de saÃ­da`.
   - **Local (Qwen 1.5B)** â€” nÃ£o exige chave; **parÃ¢metros travados** por seguranÃ§a.

2. **Escreva a pergunta** e clique **Perguntar**.

3. **Resposta** virÃ¡ com:
   - explicaÃ§Ã£o didÃ¡tica (resumo, passos, exemplo, dicas);
   - **ReferÃªncias** listando os *chunks* usados.

> Dica: Em **Local**, faÃ§a perguntas **curtas e objetivas**. O modelo tem janela menor e pode truncar entradas muito longas.

---

## ğŸ§¯ Troubleshooting

### â€œKilledâ€ (servidor Ã© encerrado)
- Sintoma tÃ­pico de **falta de RAM** (ou OOM no container).
- Use o **modo Local** com os **parÃ¢metros travados** que vÃªm no app.
- No **OpenAI**, evite `top_k` alto; a UI aplica **clamp** automÃ¡tico.
- Feche outras abas/processos pesados; em GPU use `nvidia-smi` para checar uso.

### `RuntimeError: CUDA out of memory`
- Reduza o tamanho das perguntas e **top_k**; reinicie o app apÃ³s alternar motores.
- Em mÃ¡quinas sem GPU, rode tudo em CPU (o projeto jÃ¡ faz isso automaticamente).

### â€œToken indices sequence length â€¦â€ / prompt muito longo
- O backend **trunca** pergunta/contexto, mas se insistir: reduza `top_k` e seja mais direto.

### â€œFaltou a OpenAI Key do UsuÃ¡rio.â€
- Preencha a chave em **ConfiguraÃ§Ãµes** (ou `.env`), iniciando com `sk-`.

### â€œIndexError/FAISS: no such file or directoryâ€
- Rode **na ordem correta**: `ingest.py` âœ `embed_index.py` âœ `streamlit_app.py`.

---

## ğŸ§ª Como reproduzir rÃ¡pido (comandos)

```bash
# 0) ambiente
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# 1) dados (adicione seus .txt/.md em data/sources/)

# 2) ingestÃ£o
python -m src.jarbas.ingest.ingest

# 3) Ã­ndice vetorial
python -m src.jarbas.ingest.embed_index

# (opcional) auditoria
python -m src.jarbas.ingest.audit_corpus

# 4) UI
streamlit run streamlit_app.py
```

---

## Privacidade & Custos

- **Local**: tudo roda no seu computador. Sem envios externos.
- **OpenAI**: o prompt (pergunta + contexto) Ã© enviado Ã  OpenAI. VocÃª paga **por token** de entrada/saÃ­da.
- **Chave**: a API key Ã© solicitada **apenas** quando vocÃª escolhe OpenAI no Streamlit e Ã© usada **somente na sessÃ£o**.

---

## Perguntas Frequentes (FAQ)

**1) Por que minha resposta foi curta/incompleta no Local?**  
O Qwen 1.5B tem **janela menor**. O backend aplica **truncamentos**. Tente encurtar a pergunta ou use o motor **OpenAI**.

**2) O que Ã© `top_k`?**  
Ã‰ o nÃºmero de **trechos** do seu Ã­ndice enviados no **Contexto**. Mais trechos = mais fatos, mas tambÃ©m **mais tokens** e custo/latÃªncia (OpenAI).

**3) O que causa o â€œKilledâ€?**  
Geralmente **falta de memÃ³ria** (RAM/GPU) quando a entrada fica grande demais. O app jÃ¡ limita isso, mas use valores conservadores.

**4) Posso usar outro modelo local?**  
Sim. Ajuste `GEN_MODEL` no `local_backend.py` e assegure-se de que a **janela** e **VRAM** comportam o modelo.

**5) Posso usar outros provedores alÃ©m da OpenAI?**  
A arquitetura permite, mas vocÃª precisarÃ¡ implementar um backend anÃ¡logo (`*_backend.py`) para o provedor desejado.

---
## ğŸ“„ LicenÃ§a & crÃ©ditos

Projeto feito para **fins educacionais**. 
CÃ³digo liberado sob licenÃ§a **MIT** 
Modelos e pacotes externos seguem as **suas prÃ³prias licenÃ§as** (consulte os repositÃ³rios).

Feito por **Fillipe Berssot** como **projeto de estudo**.  
Ideias e ajustes de prompt/pipeline foram inspirados pela documentaÃ§Ã£o das libs usadas e por boas prÃ¡ticas comuns em RAG.