import time
import streamlit as st

from src.jarbas.rag.openai_backend import RAGTeacherOpenAI
from src.jarbas.rag.local_backend import RAGTeacher


# Constantes de Seguran√ßa para o Modo Local
LOCAL_TOP_K = 4
LOCAL_TEMPERATURE = 0.2
LOCAL_MAX_OUTPUT_TOKENS = 320

OPENAI_DEFAULT_TOP_K = 5
SAFE_TOPK_MAX_OPENAI = 7

# Configura√ß√£o da P√°gina
st.set_page_config(
    page_title="Jarbas ‚Ä¢ O Seu Professor de Programa√ß√£o - Python",
    page_icon="ü§ñ",
    layout="wide",
)

# estado de sess√£o para saber quando o motor muda 
if "active_engine" not in st.session_state:
    st.session_state.active_engine = None
if "local_instance" not in st.session_state:
    st.session_state.local_instance = None 

# Sidebar: Configura√ß√µes
with st.sidebar:
    st.header("‚öôÔ∏è Configura√ß√µes")

    engine = st.selectbox(
        "Motor de gera√ß√£o",
        ["OpenAI (gpt-4o-mini)", "Local (Qwen/Qwen2.5-0.5B-Instruct)"],
        help = (
            "‚Ä¢ OpenAI: janela maior, respostas mais completas, requer API key.\n"
            "‚Ä¢ Local (Qwen2.5 - 0.5B): roda no seu hardware, sem chave; √© mais limitado "
            "em contexto e sa√≠da."
        ) ,
    )

    # OPENAI: mostrar campos edit√°veis
    if engine.startswith("OpenAI"):
        user_api_key = st.text_input(
            "Sua OpenAI API KEY",
            type="password",
            placeholder="sk-...",
            help="Sua chave √© usada apenas nesta sess√£o.",
        )

        top_k = st.slider(
            "top_k (trechos recuperados)",
            1, 10, OPENAI_DEFAULT_TOP_K,
            help=(
                f"Quantos trechos do seu √≠ndice entram no CONTEXTO. "
                f"Valores maiores ‚Üë trazem mais fatos, mas aumentam custo/lat√™ncia e podem encerrar a sess√£o ('Killed'). "
                f"Recomendado ‚â§ {SAFE_TOPK_MAX_OPENAI}"
            ),
        )

        # Clamp + aviso
        effective_top_k = min(top_k, SAFE_TOPK_MAX_OPENAI)
        if top_k > SAFE_TOPK_MAX_OPENAI:
            st.warning(f"Para estabilidade, limitei o top_k de {top_k} -> {effective_top_k}")

        temperature = st.slider(
            "temperatura",
            0.0, 1.0, 0.2, 0.1,
            help=(
                "Controle de criatividade da gera√ß√£o. 0.0 = mais determin√≠stico; 1.0 = mais criativo. "
                "Para respostas t√©cnicas, 0.1‚Äì0.3 costuma funcionar bem."
            ),
        )

        max_output_tokens = st.slider(
            "tokens de sa√≠da (aprox.)",
            64, 4000, 700, 64,
            help=(
                "Limite de tokens gerados na RESPOSTA. Aumentar permite respostas mais longas, "
                "mas custa mais (OpenAI) e pode ficar verboso."
            ),
        )

    # LOCAL: travar op√ß√µes (valores fixos)
    else:
        user_api_key = None  # n√£o usado
        # Mostrar como desabilitado, s√≥ para o usu√°rio ver o que est√° valendo:
        st.slider(
            "top_k (trechos recuperados)",
            1, 10, LOCAL_TOP_K, disabled=True,
            help=(
                "No modo LOCAL os par√¢metros ficam travados para evitar 'Killed' e estouros de mem√≥ria. "
                f"Valor fixo: {LOCAL_TOP_K}."
            ),
        )
        st.slider(
            "temperatura",
            0.0, 1.0, LOCAL_TEMPERATURE, 0.1, disabled=True,
            help="Travado no modo LOCAL. Valor informativo.",
        )
        st.slider(
            "tokens de sa√≠da (aprox.)",
            64, 2000, LOCAL_MAX_OUTPUT_TOKENS, 64, disabled=True,
            help=(
                "Travado no modo LOCAL. O backend tamb√©m limita dinamicamente para caber na janela do modelo."
            ),
        )

        # Instru√ß√µes extras do modo local
        st.info(
            "üñ•Ô∏è **Modo Local (Qwen2.5 - 0.5B)**\n\n"
            "- Ideal para testes sem API key.\n"
            "- Contexto curto (janela menor). Perguntas e respostas muito longas podem ser encurtadas.\n"
            "- Evite colar textos gigantes na pergunta.\n"
            "- Se precisar de respostas mais extensas, selecione o motor **OpenAI**."
        )

# detectar mudan√ßa de motor e limpar recursos pesados
def _cleanup_on_switch():
    # se t√≠nhamos um local carregado, liberar recursos
    loc = st.session_state.get("local_instance")
    if loc is not None:
        try:
            loc.release()
        except Exception:
            pass
        st.session_state.local_instance = None
    # limpar recursos cacheados do streamlit (inclusive modelos)
    st.cache_resource.clear()
    # coletor + GPU
    import gc, torch
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

# engine √© a vari√°vel que voc√™ j√° define na sidebar
if st.session_state.active_engine is None:
    st.session_state.active_engine = engine
elif st.session_state.active_engine != engine:
    _cleanup_on_switch()
    st.session_state.active_engine = engine

import torch
if torch.cuda.is_available():
    torch.cuda.empty_cache()

# Cache: carregar modelos/√≠ndice s√≥ uma vez
@st.cache_resource(show_spinner=True)
def load_teacher_local():
    t0 = time.time()
    teacher = RAGTeacher(top_k=LOCAL_TOP_K)
    return teacher, (time.time() - t0)

@st.cache_resource(show_spinner=True)
def load_teacher_openai(top_k_value: int, api_key: str):
    t0 = time.time()
    teacher = RAGTeacherOpenAI(top_k=top_k_value, api_key=api_key)
    return teacher, (time.time() - t0)

# Caixa de entrada
st.subheader("Jarbas ‚Ä¢ O Seu Professor de Programa√ß√£o - Python")
st.subheader("Fa√ßa sua pergunta")

question = st.text_area(
    " ", 
    height=100, 
    placeholder="Escreva aqui...",
    label_visibility="collapsed"
)
answer = None

# Dicas espec√≠ficas para o modo local (expander na √°rea central)
if not "engine" in locals() or not engine.startswith("OpenAI"):
    with st.expander("üí° Dicas para o modo local (Qwen2.5 - 0.5B)"):
        st.markdown(
            """
- Fa√ßa **perguntas objetivas e curtas** (1‚Äì3 frases).
- Prefira **t√≥picos espec√≠ficos** (ex.: ‚ÄúComo criar uma rota POST no FastAPI com Pydantic?‚Äù).
- Evite colar manuais enormes na pergunta ‚Äî o **contexto** j√° vem do √≠ndice.
- Se a resposta vier incompleta, **refa√ßa** a pergunta de forma mais direta.
- Para conte√∫dos longos, **use OpenAI** na aba de configura√ß√µes.
            """
        )       

col_run1, col_run2 = st.columns([1, 2])
with col_run1:
    run = st.button("Perguntar", type="primary")
with col_run2:
    st.write("")

# Execu√ß√£o
if run:
    # Valida√ß√µes antes de carregar/rodar
    if not question or not question.strip():
        st.warning("Escreva uma pergunta antes de continuar.")
        st.stop()

    if not engine.startswith("OpenAI") and len(question) > 1200:
        st.info("Sua pergunta √© bem longa. No modo **Local** eu posso encurt√°-la para caber na janela do modelo.")

    # Carregamento do backend
    if engine.startswith("OpenAI"):
        if not user_api_key or not user_api_key.strip().startswith("sk-"):
            st.error("Informe sua OpenAI API Key (formato 'sk-...').")
            st.stop()
        with st.spinner("Carregando √≠ndice/modelo (OpenAI)..."):
            teacher, load_secs = load_teacher_openai(
                effective_top_k,
                user_api_key.strip()
            )
        st.success(f"Pronto em {load_secs:.2f}s ‚Ä¢ top_k={effective_top_k}")
    else:
        with st.spinner("Carregando √≠ndice/modelo (Local)..."):
            teacher, load_secs = load_teacher_local()
            st.session_state.local_instance = teacher  # manter refer√™ncia p/ release()
        st.success(f"Pronto em {load_secs:.2f}s ‚Ä¢ Modo Local ‚Ä¢ top_k={LOCAL_TOP_K}")

    # Gera√ß√£o
    with st.spinner("Gerando resposta..."):
        try:
            if engine.startswith("OpenAI"):
                answer = teacher.ask(
                    question,
                    temperature=float(temperature),
                    max_output_tokens=int(max_output_tokens),
                )
            else:
                answer = teacher.ask(question)
        except Exception as e:
            st.error(f"Ops! Algo deu errado ao gerar a resposta: {e}")
            answer = None

# Render da resposta (fora do if run, mas protegido)
if answer:
    st.divider()
    st.subheader("Resposta")
    st.write(answer)

    st.divider()
    st.markdown(
        """
        **Sobre este projeto**

        Este √© um projeto **educacional** feito para estudo de RAG e interfaces.  
        N√£o √© um produto profissional e **pode apresentar limita√ß√µes ou erros** ‚Äî principalmente no modo **Local**, que usa um modelo pequeno e roda no seu hardware.

        Se voc√™ precisa de respostas mais completas e est√°veis, utilize o motor **OpenAI**.
        """
    )
    st.caption("Jarbas ‚Ä¢ RAG nos seus dados com gera√ß√£o OpenAI ou local")

    